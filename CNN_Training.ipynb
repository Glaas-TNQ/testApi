{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426cd373",
   "metadata": {},
   "source": [
    "Model Training v_0.1\n",
    "Faretra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d680e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "from io import FileIO\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e025b",
   "metadata": {},
   "source": [
    "Check for CUDA (gpu augmentation), if the model has to use the CPU resources of the machine \n",
    "then the training and prediction time will be greatly increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005260d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb5b06",
   "metadata": {},
   "source": [
    "For testing purposes we're gonna use Intel Img Classification dataset, but we're gonna add an horizontal flip at random to \n",
    "every img presented to the model in order to increase the variance without the need to download more files. \n",
    "Vertical flip would be useless here, as natural features such as trees are never vertically flipped. \n",
    "It may prove useful for the actual clothing dataset, as clothes come in different form, shape \n",
    "and sometimes orientation in space.\n",
    "We're also going to trasform the pixel rgb value in a tensor array and normalize it in order to use the resulting tensor to make prediction (computer vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e0e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer= transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize ([0.5,0.5,0.5], #0-1 to [-1,1] , formula(x-mean)/std\n",
    "                         [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede92382",
   "metadata": {},
   "source": [
    "\n",
    "Dataloader\n",
    "\n",
    "\n",
    "It is common best practice, in order to avoid memory overload, to not upload all the training img dataset at once, but divide it into serveral different batches. It's useful to adapt batch size relative to the CPU or GPU performaces in order to avoid memory overload errors.\n",
    "In the Dataloader() we're also adding a shuffle to the batch in order for the model not to be biased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10889ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=r\"C:/Users/LT_J/OneDrive/Desktop/ML/InnovatesApp/Archivio_Img/seg_train\"\n",
    "test_path=r\"C:/Users/LT_J/OneDrive/Desktop/ML/InnovatesApp/Archivio_Img/seg_test\"\n",
    "#C:\\Users\\LT_J\\OneDrive\\Desktop\\ML\\InnovatesApp\\Archivio_Img\\seg_test\n",
    "#C:/Users/LT_J/OneDrive/Desktop/ML/InnovatesApp/Archivio_Img/seg_train\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path, transform=transformer),\n",
    "    batch_size=256, shuffle =True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path, transform=transformer),\n",
    "    batch_size=256, shuffle =True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988abcf",
   "metadata": {},
   "source": [
    "Now we're going to extract all the different categories from the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a9b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classes are\n",
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
      "\n",
      "Test classes are\n",
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
     ]
    }
   ],
   "source": [
    "train_root=pathlib.Path(str(train_path))\n",
    "test_root=pathlib.Path(str(test_path))\n",
    "train_classes= os.listdir(train_root)\n",
    "test_classes= os.listdir(test_root)\n",
    "\n",
    "print(\"Training classes are\")\n",
    "print(train_classes)\n",
    "print('')\n",
    "print(\"Test classes are\")\n",
    "print(test_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12089456",
   "metadata": {},
   "source": [
    "Now we'll write our CNN Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26cbc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=6):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1  w=Width,Heigth, f=kernelSize , P=padding , s=stride\n",
    "        #Input shape= (256,3,150,150)--->(batchSize,colorChannels,Xsize,Ysize)\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #Output shape (256,12,150,150)-->increased channel number\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)  #n_features=channels\n",
    "        #Output shape (256,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Output shape (256,12,150,150)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size by factor 2\n",
    "        #Output shape (256,12,75,75)\n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Output shape (256,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Output shape (256,20,75,75)\n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Output shape (256,32,75,75)-->increased channel number\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)  #n_features=channels\n",
    "        #Output shape (256,32,75,75)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Output shape (256,32,75,75)\n",
    "\n",
    "        \n",
    "        #The syntax we saw on the previous lines is how we can add multiple convolution layers to our model.\n",
    "        #By adding more steps that gradually increase the number of channels and the accuracy fo the model\n",
    "        \n",
    "        self.fc=nn.Linear(in_features=32*75*75,out_features=num_classes)\n",
    "        \n",
    "        #Feed forward Function\n",
    "        \n",
    "\n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "            #Output is going to be in matrix form(256,32,75,75), \n",
    "            #to feed the output we're going to reshape it first\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec58570",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvNet(num_classes=6).to(device)\n",
    "model=ConvNet(num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d4986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549c353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0b55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b67ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14034 3000\n"
     ]
    }
   ],
   "source": [
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9cc867",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0Train Loss 10 Train Accuracy: 0.5106883283454468 Test Accuracy 0.6066666666666667\n",
      "Epoch: 1Train Loss 1 Train Accuracy: 0.7034345161750035 Test Accuracy 0.6166666666666667\n",
      "Epoch: 2Train Loss 1 Train Accuracy: 0.778395325637737 Test Accuracy 0.7166666666666667\n",
      "Epoch: 3Train Loss 0 Train Accuracy: 0.8321932449764857 Test Accuracy 0.6413333333333333\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model for each epoch. We're gonna save \n",
    "#the best model for each epoch\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    #Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    \n",
    "    for i,(images,labels) in enumerate(test_loader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "        \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    print('Epoch: '+ str(epoch)+ 'Train Loss '+str(int(train_loss))+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy '+ str(test_accuracy))\n",
    "    \n",
    "    #Save best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02b1c1",
   "metadata": {},
   "source": [
    "We will save our trained model to the disk using the pickle library. Pickle is used to serializing and de-serializing a Python object structure. In which python object is converted into the byte stream. dump() method dumps the object into the file specified in the arguments.\n",
    "\n",
    "In our case, we want to save our model so that it can be used by the server. So we will save our object regressor to the file named model.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922a9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model.pkl','wb'))\n",
    "pickle.dump(model,open('model.pkl2','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
